% Assignment 1 report — Comparative analysis of synthetic vs human datasets
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Assignment 1: Comparative analysis of synthetic vs human datasets}
\author{Usman Amjad (analysis) \\ Generated by analysis script}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This short report summarizes a basic comparative analysis between the human-annotated development dataset (`dev_track_a.jsonl`) and the combined synthetic dataset (`combined_synthetic_for_training.jsonl`). The analysis covers dataset sizes, token-length distributions, vocabulary sizes, and the top unigrams. Results and CSV outputs were generated with `assignment_1/assignment_1_compare.py` and are saved in `reports/assignment_1/`.
\end{abstract}

\section{Datasets}
\begin{itemize}
  \item Human dev set: \texttt{data/dev_track_a.jsonl} (pairs of texts `text\_a` / `text\_b`, label `text\_a\_is\_closer`).
  \item Synthetic combined: \texttt{data/combined_synthetic_for_training.jsonl} (anchor / similar / dissimilar generated stories from multiple model sources).
\end{itemize}

\section{Methods}
We computed the following summary statistics for each dataset:
\begin{enumerate}
\item Number of texts (total concatenated candidate texts used in analysis).
\item Token-level statistics: mean, median, and population standard deviation of tokens per text (tokens derived with a simple word regex \texttt{\textbackslash w+}).
\item Vocabulary size (unique token types observed across all texts).
\item Top unigrams, bigrams and trigrams (frequency counts).
\end{enumerate}

All code used is in `assignment_1/assignment_1_compare.py`. The script uses a lightweight tokenizer (word characters) for portability.

\section{Key results}
\subsection{Summary statistics}
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
Dataset & \# texts & Avg tokens & Median tokens & Vocab size \\
\midrule
Human dev & 400 & 123.28 & 122 & 7,094 \\
Synthetic combined & 7,191 & 157.91 & 158 & 18,835 \\
\bottomrule
\end{tabular}
\caption{High-level summary statistics (token counts and vocabulary).}
\end{table}

Observations:
\begin{itemize}
  \item The combined synthetic set is much larger (7,191 texts vs 400), and exhibits a higher mean token length and much larger vocabulary.
  \item The synthetic data's average tokens are ~158 versus ~123 for the human dev set — synthetic texts are on average longer.
\end{itemize}

\subsection{Top unigrams (top 10)}
\begin{minipage}{0.48\textwidth}
\centering
\textbf{Human dev — top unigrams}
\begin{tabular}{lr}
\toprule
Unigram & Count \\
\midrule
the & 2,861 \\
a & 1,780 \\
to & 1,656 \\
and & 1,512 \\
of & 1,233 \\
in & 951 \\
is & 886 \\
his & 828 \\
he & 649 \\
with & 560 \\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\centering
\textbf{Synthetic combined — top unigrams}
\begin{tabular}{lr}
\toprule
Unigram & Count \\
\midrule
the & 92,919 \\
a & 52,562 \\
to & 38,655 \\
of & 33,426 \\
and & 30,940 \\
in & 18,345 \\
s & 15,422 \\
her & 13,154 \\
as & 12,648 \\
with & 11,647 \\
\bottomrule
\end{tabular}
\end{minipage}

\section{Short discussion}
The synthetic data shows much larger absolute token counts (expected for a larger corpus) and a larger vocabulary. A few caveats:
\begin{itemize}
  \item The synthetic corpus is aggregated from diverse model sources; this inflates vocabulary and frequency counts compared to the smaller, more focused human dev set.
  \item Tokenization is simple (word regex) and counts will vary slightly with different tokenizers (e.g., spaCy, NLTK).
  \item High-frequency tokens (stopwords like ``the'', ``a'', ``to'') dominate both corpora; meaningful lexical differences require analyzing content words, named entities, or normalized frequencies (e.g., per-1k tokens).
\end{itemize}

\section{Files and reproducibility}
CSV outputs generated by the analysis script are in `reports/assignment_1/`:
\begin{itemize}
  \item `human_dev_summary.csv`, `synthetic_combined_summary.csv`
  \item `human_dev_top_unigrams.csv`, `synthetic_combined_top_unigrams.csv`
  \item corresponding bigrams/trigrams CSV files
\end{itemize}

To reproduce: from the repository root run:
\begin{verbatim}
python assignment_1/assignment_1_compare.py
\end{verbatim}

\section{Next steps (optional)}
Possible extensions:
\begin{itemize}
  \item Generate normalized frequency comparisons (tokens per 1k) and lexical overlap (Jaccard) between datasets.
  \item Compute readability scores (Flesch–Kincaid), POS distributions, and named-entity overlap.
  \item Add plots (histograms, word-frequency bars). I can add these and commit PNGs if you want.
\end{itemize}

\vfill
\small{Report generated automatically from `assignment_1/assignment_1_compare.py` outputs.}
\end{document}
